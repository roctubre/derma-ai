{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derma AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define datesets and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from torchvision import datasets\n",
    "from PIL import ImageFile\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Data Loaders for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data:\t 2000\n",
      "Valid. data:\t 150\n",
      "Test data:\t 600\n"
     ]
    }
   ],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 16\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# image size (side of square)\n",
    "image_size = 224\n",
    "\n",
    "# set data paths\n",
    "data_dir = \"data/\"\n",
    "train_dir = os.path.join(data_dir, 'train/')\n",
    "valid_dir = os.path.join(data_dir, 'valid/')\n",
    "test_dir = os.path.join(data_dir, 'test/')\n",
    "\n",
    "# define transformation pipelines\n",
    "train_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.RandomResizedCrop(image_size),\n",
    "                                      transforms.RandomRotation(30),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "\n",
    "basic_transform = transforms.Compose([transforms.Resize(256),\n",
    "                                       transforms.CenterCrop(image_size),\n",
    "                                       transforms.ToTensor(),\n",
    "                                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                                         std=[0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "\n",
    "# define datasets\n",
    "data = {\n",
    "    \"train\": datasets.ImageFolder(train_dir, transform=train_transform),\n",
    "    \"valid\": datasets.ImageFolder(valid_dir, transform=basic_transform),\n",
    "    \"test\": datasets.ImageFolder(test_dir, transform=basic_transform)\n",
    "}\n",
    "\n",
    "# prepare data loaders\n",
    "loaders = {\n",
    "    \"train\": torch.utils.data.DataLoader(data[\"train\"], batch_size=batch_size, \n",
    "                                           num_workers=num_workers, shuffle=True),\n",
    "    \"valid\": torch.utils.data.DataLoader(data[\"valid\"], batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=True),\n",
    "    \"test\": torch.utils.data.DataLoader(data[\"test\"], batch_size=batch_size, \n",
    "                                          num_workers=num_workers, shuffle=True)\n",
    "}\n",
    "\n",
    "print(\"Train data:\\t\", len(loaders[\"train\"].dataset))\n",
    "print(\"Valid. data:\\t\", len(loaders[\"valid\"].dataset))\n",
    "print(\"Test data:\\t\", len(loaders[\"test\"].dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print(\"CUDA is available\")\n",
    "else:\n",
    "    print(\"CUDA NOT available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 224, 224]           1,792\n",
      "       BatchNorm2d-2         [-1, 64, 224, 224]             128\n",
      "            Conv2d-3         [-1, 64, 224, 224]          36,928\n",
      "       BatchNorm2d-4         [-1, 64, 224, 224]             128\n",
      "              ReLU-5         [-1, 64, 224, 224]               0\n",
      "         MaxPool2d-6         [-1, 64, 112, 112]               0\n",
      "            Conv2d-7        [-1, 128, 112, 112]          73,856\n",
      "       BatchNorm2d-8        [-1, 128, 112, 112]             256\n",
      "            Conv2d-9        [-1, 128, 112, 112]         147,584\n",
      "      BatchNorm2d-10        [-1, 128, 112, 112]             256\n",
      "             ReLU-11        [-1, 128, 112, 112]               0\n",
      "        MaxPool2d-12          [-1, 128, 56, 56]               0\n",
      "           Conv2d-13          [-1, 256, 56, 56]         295,168\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "           Conv2d-15          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-16          [-1, 256, 56, 56]             512\n",
      "           Conv2d-17          [-1, 256, 56, 56]         590,080\n",
      "      BatchNorm2d-18          [-1, 256, 56, 56]             512\n",
      "             ReLU-19          [-1, 256, 56, 56]               0\n",
      "        MaxPool2d-20          [-1, 256, 28, 28]               0\n",
      "           Conv2d-21          [-1, 512, 28, 28]       1,180,160\n",
      "      BatchNorm2d-22          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-23          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-24          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-25          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-26          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-27          [-1, 512, 28, 28]       2,359,808\n",
      "      BatchNorm2d-28          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-29          [-1, 512, 28, 28]               0\n",
      "        MaxPool2d-30          [-1, 512, 14, 14]               0\n",
      "           Linear-31                  [-1, 512]      51,380,736\n",
      "             ReLU-32                  [-1, 512]               0\n",
      "      BatchNorm1d-33                  [-1, 512]           1,024\n",
      "          Dropout-34                  [-1, 512]               0\n",
      "           Linear-35                  [-1, 256]         131,328\n",
      "             ReLU-36                  [-1, 256]               0\n",
      "      BatchNorm1d-37                  [-1, 256]             512\n",
      "          Dropout-38                  [-1, 256]               0\n",
      "           Linear-39                   [-1, 64]          16,448\n",
      "             ReLU-40                   [-1, 64]               0\n",
      "      BatchNorm1d-41                   [-1, 64]             128\n",
      "          Dropout-42                   [-1, 64]               0\n",
      "           Linear-43                    [-1, 3]             195\n",
      "================================================================\n",
      "Total params: 61,531,843\n",
      "Trainable params: 61,531,843\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 265.70\n",
      "Params size (MB): 234.73\n",
      "Estimated Total Size (MB): 501.00\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    ### TODO: choose an architecture, and complete the class\n",
    "    def __init__(self, input_size, output_nodes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_nodes = output_nodes\n",
    "        \n",
    "        ## Define layers of a CNN\n",
    "        \n",
    "        # Size 224\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.conv1_bn = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Size 112\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.conv2_bn = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Size 56\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv3_bn = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Size 28\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv4_bn = nn.BatchNorm2d(512)\n",
    "\n",
    "        \n",
    "        # Fully connected layers, Input: 14\n",
    "        self.fc1 = nn.Linear(14*14*512, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, output_nodes)\n",
    "        \n",
    "        self.fc1_bn = nn.BatchNorm1d(512)\n",
    "        self.fc2_bn = nn.BatchNorm1d(256)\n",
    "        self.fc3_bn = nn.BatchNorm1d(64)\n",
    "        \n",
    "        # Pooling function\n",
    "        self.pool = nn.MaxPool2d(2,2)\n",
    "        \n",
    "        # activation and dropout function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## Define forward behavior\n",
    "\n",
    "        # feature layers\n",
    "        x = self.conv1_bn(self.conv1_1(x))\n",
    "        x = self.conv1_bn(self.conv1_2(x))\n",
    "        x = self.pool(self.relu(x))\n",
    "        x = self.conv2_bn(self.conv2_1(x))\n",
    "        x = self.conv2_bn(self.conv2_2(x))\n",
    "        x = self.pool(self.relu(x))\n",
    "        x = self.conv3_bn(self.conv3_1(x))\n",
    "        x = self.conv3_bn(self.conv3_2(x))\n",
    "        x = self.conv3_bn(self.conv3_3(x))\n",
    "        x = self.pool(self.relu(x))\n",
    "        x = self.conv4_bn(self.conv4_1(x))\n",
    "        x = self.conv4_bn(self.conv4_2(x))\n",
    "        x = self.conv4_bn(self.conv4_3(x))\n",
    "        x = self.conv4_bn(self.conv4_4(x))\n",
    "        x = self.pool(self.relu(x))\n",
    "        \n",
    "        # flatten tensor\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        # classifier\n",
    "        \n",
    "        x = self.dropout(self.fc1_bn(self.relu(self.fc1(x))))\n",
    "        x = self.dropout(self.fc2_bn(self.relu(self.fc2(x))))\n",
    "        x = self.dropout(self.fc3_bn(self.relu(self.fc3(x))))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#-#-# You do NOT have to modify the code below this line. #-#-#\n",
    "\n",
    "# instantiate the CNN\n",
    "model = Net(image_size, len(data[\"train\"].classes))\n",
    "\n",
    "# move tensors to GPU if CUDA is available, print out model summary\n",
    "if use_cuda:\n",
    "    model.cuda()\n",
    "    summary(model.cuda(), (3, image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path, \n",
    "          max_epochs_not_improved = 10):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    # Quits training after valid_loss_min not improving after X consecutive epochs\n",
    "    #max_epochs_not_improved = 5\n",
    "    \n",
    "    # running counter\n",
    "    cnt_epochs_not_improved = 0\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # timer for epoch\n",
    "        start = time.time()\n",
    "        \n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "            \n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            ## find the loss and update the model parameters accordingly\n",
    "            ## record the average training loss, using something like\n",
    "            ## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))\n",
    "            \n",
    "            # clear the gradients of all optimized variables\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # backward pass: compute gradient\n",
    "            loss.backward()\n",
    "            # update parameters\n",
    "            optimizer.step()\n",
    "            # update running training loss\n",
    "            train_loss += (1 / (batch_idx + 1)) * (loss.data - train_loss)\n",
    "            #train_loss += loss.item()\n",
    "            \n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "                \n",
    "            ## update the average validation loss\n",
    "            \n",
    "            # forward pass\n",
    "            output = model(data)\n",
    "            # calculate the loss\n",
    "            loss = criterion(output, target)\n",
    "            # update running validation loss\n",
    "            valid_loss += (1 / (batch_idx + 1)) * (loss.data - valid_loss)\n",
    "            #valid_loss += loss.item()\n",
    "            \n",
    "        #train_loss = train_loss/len(loaders[\"train\"])\n",
    "        #valid_loss = valid_loss/len(loaders[\"valid\"])\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {}\\tTraining Loss: {:.6f}\\tValidation Loss: {:.6f}\\tTime: {:.3f} seconds'\n",
    "              .format(epoch, \n",
    "                      train_loss,\n",
    "                      valid_loss,\n",
    "                      time.time() - start\n",
    "                     ))\n",
    "        \n",
    "        ## TODO: save the model if validation loss has decreased\n",
    "        if valid_loss < valid_loss_min:\n",
    "            print('Validation loss decreased. Saving model...')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "            cnt_epochs_not_improved = 0\n",
    "        else:\n",
    "            # Adds to count if model didn't improve\n",
    "            # Quits training if threshold reached\n",
    "            cnt_epochs_not_improved += 1\n",
    "            if(cnt_epochs_not_improved == max_epochs_not_improved):\n",
    "                print(\"Validation loss not decreasing anymore. Training stopped.\")\n",
    "                break\n",
    "\n",
    "    # return trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\tTraining Loss: 1.039518\tValidation Loss: 1.063918\tTime: 49.801 seconds\n",
      "Validation loss decreased. Saving model...\n",
      "Epoch: 2\tTraining Loss: 0.889277\tValidation Loss: 1.097090\tTime: 49.782 seconds\n",
      "Epoch: 3\tTraining Loss: 0.841708\tValidation Loss: 1.099353\tTime: 48.868 seconds\n",
      "Epoch: 4\tTraining Loss: 0.798273\tValidation Loss: 1.125645\tTime: 48.032 seconds\n",
      "Epoch: 5\tTraining Loss: 0.792644\tValidation Loss: 1.113659\tTime: 47.052 seconds\n",
      "Epoch: 6\tTraining Loss: 0.800695\tValidation Loss: 1.171663\tTime: 50.529 seconds\n",
      "Epoch: 7\tTraining Loss: 0.785728\tValidation Loss: 1.114463\tTime: 49.463 seconds\n",
      "Epoch: 8\tTraining Loss: 0.777107\tValidation Loss: 1.109518\tTime: 48.473 seconds\n",
      "Epoch: 9\tTraining Loss: 0.773456\tValidation Loss: 1.175651\tTime: 47.600 seconds\n",
      "Epoch: 10\tTraining Loss: 0.770419\tValidation Loss: 1.154363\tTime: 48.065 seconds\n",
      "Epoch: 11\tTraining Loss: 0.765249\tValidation Loss: 1.211368\tTime: 49.334 seconds\n",
      "Validation loss not decreasing anymore. Training stopped.\n"
     ]
    }
   ],
   "source": [
    "# train the model\n",
    "model = train(50, loaders, model, optimizer, \n",
    "                      criterion, use_cuda, 'model_derma.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IncompatibleKeys(missing_keys=[], unexpected_keys=[])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model that got the best validation accuracy\n",
    "model.load_state_dict(torch.load('model_derma.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.968527\tTest Accuracy: 65.5 % (393/600)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    model.eval()\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            model.cuda()\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\tTest Accuracy: {:.1f} % ({}/{})'\n",
    "              .format(test_loss, 100. * correct / total, int(correct), int(total)))\n",
    "\n",
    "# call test function    \n",
    "test(loaders, model, criterion, use_cuda)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
